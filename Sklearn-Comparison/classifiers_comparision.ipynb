{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38sZcNCNxCng",
        "outputId": "c3ea8ecb-d6ab-4ef7-e302-8dad46f74e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random segments at - 4206 14850 24698\n",
            "BernoulliNB - 0.79\n",
            "MultinomialNB - 0.8633333333333333 0.86\n",
            "KNN - 0.45 0.5766666666666667 0.8266666666666667\n",
            "Rocchio - 0.6 0.5966666666666667 0.7\n",
            "Logistic Regression - 0.7966666666666666 0.7833333333333333 0.7933333333333333\n",
            "LinearSVC - 0.8166666666666667 0.7966666666666666 0.8366666666666667\n",
            "SVC - 0.7433333333333333 0.76 0.8633333333333333\n",
            "DecisionTreeClassifier - 0.6666666666666666 0.6633333333333333 0.6433333333333333\n",
            "ExtraTreeClassifier - 0.6433333333333333 0.6166666666666667 0.63\n",
            "ExtraTreesClassifier - 0.83 0.83 0.8333333333333334\n",
            "AdaBoostClassifier - 0.64 0.64 0.6133333333333333\n",
            "RandomForestClassifier - 0.8566666666666667 0.8033333333333333 0.7633333333333333\n",
            "Perceptron - 0.81 0.7833333333333333 0.85\n",
            "MLPClassifier - 0.8566666666666667 0.8433333333333334 0.8266666666666667\n"
          ]
        }
      ],
      "source": [
        "# IR16A.py CS5154/6054 cheng 2022\n",
        "# Comparing classifiers on documents as binary, count, or tfidf vector\n",
        "# on three random segments of bible.txt from the first, third, and last fifths\n",
        "# 100 test documents are at the center of 1000 training documents\n",
        "# Only the four in IIR Chapters 13 and 14 are implemented\n",
        "# you need to add the ten others imported like you did for IR15A.py\n",
        "# Usage: python IR16A.py\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "f = open(\"bible.txt\", \"r\")\n",
        "docs = f.readlines()\n",
        "f.close()\n",
        "N = len(docs)\n",
        "N1 = N // 5 - 1100\n",
        "c0 = random.randrange(N1)\n",
        "c1 = 2 * N // 5 + random.randrange(N1)\n",
        "c2 = N - 1100 - random.randrange(N1)\n",
        "print('Random segments at -', c0, c1, c2)\n",
        "\n",
        "trainX = np.concatenate([docs[c0:c0 + 500], docs[c0 + 600:c0 + 1100],\n",
        "                         docs[c1:c1 + 500], docs[c1 + 600:c1 + 1100], docs[c2:c2 + 500], docs[c2 + 600:c2 + 1100]])\n",
        "y = np.concatenate([np.zeros(1000, dtype=np.int16), np.ones(1000, dtype=np.int16), np.full(1000, 2, dtype=np.int16)])\n",
        "testX = np.concatenate([docs[c0 + 500:c0 + 600], docs[c1 + 500:c1 + 600], docs[c2 + 500:c2 + 600]])\n",
        "testY = np.concatenate([np.zeros(100, dtype=np.int16), np.ones(100, dtype=np.int16), np.full(100, 2, dtype=np.int16)])\n",
        "\n",
        "# documents as binary vectors\n",
        "cv = CountVectorizer(binary=True, max_df=0.4, min_df=4)\n",
        "X0 = cv.fit_transform(trainX).toarray()\n",
        "T0 = cv.transform(testX).toarray()\n",
        "\n",
        "# documents as count vectors\n",
        "cv = CountVectorizer(max_df=0.4, min_df=4)\n",
        "X1 = cv.fit_transform(trainX).toarray()\n",
        "T1 = cv.transform(testX).toarray()\n",
        "\n",
        "# documents as tfidf vectors\n",
        "cv = TfidfVectorizer(max_df=0.4, min_df=4)\n",
        "X2 = cv.fit_transform(trainX).toarray()\n",
        "T2 = cv.transform(testX).toarray()\n",
        "\n",
        "model = BernoulliNB()\n",
        "model.fit(X0, y)\n",
        "pred = model.predict(T0)\n",
        "A0 = accuracy_score(testY, pred)\n",
        "print('BernoulliNB -', A0)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "print('MultinomialNB -', A0, A1)\n",
        "\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('KNN -', A0, A1, A2)\n",
        "\n",
        "model = NearestCentroid()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('Rocchio -', A0, A1, A2)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('Logistic Regression -', A0, A1, A2)\n",
        "\n",
        "model = LinearSVC()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('LinearSVC -', A0, A1, A2)\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('SVC -', A0, A1, A2)\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('DecisionTreeClassifier -', A0, A1, A2)\n",
        "\n",
        "model = ExtraTreeClassifier()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('ExtraTreeClassifier -', A0, A1, A2)\n",
        "\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('ExtraTreesClassifier -', A0, A1, A2)\n",
        "\n",
        "model = AdaBoostClassifier()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('AdaBoostClassifier -', A0, A1, A2)\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('RandomForestClassifier -', A0, A1, A2)\n",
        "\n",
        "model = Perceptron()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('Perceptron -', A0, A1, A2)\n",
        "\n",
        "model = MLPClassifier()\n",
        "model.fit(X0, y)\n",
        "A0 = accuracy_score(testY, model.predict(T0))\n",
        "model.fit(X1, y)\n",
        "A1 = accuracy_score(testY, model.predict(T1))\n",
        "model.fit(X2, y)\n",
        "A2 = accuracy_score(testY, model.predict(T2))\n",
        "print('MLPClassifier -', A0, A1, A2)\n"
      ]
    }
  ]
}